Toward Measuring Visualization Insight,,,North,,,Chris,,,12,,,Insight: The capacity to discern the true nature of a situation; The act or outcome of grasping the inward or hidden nature of things or of perceiving in an intuitive manner. -Merriam-Webster,0,3854.603,4828.603p,,,Recent visualization research literature has paid a good amount of attention to evaluating visualizations.,1,6376.12,7350.12"For example, this trend was evident at the 2005 IEEE Visualization conference where many of the presentations included evaluation components.",0,4245.215,5219.215The conference provided some inspiring presentations that probed the philosophical boundaries of visualization and evaluation.,0,10047.119,11021.119"Unfortunately, there were also a few too many Boolean usability studies that offered only two alternatives: either the users liked the visualization tool in question, or they did not.",1,3754.482,4728.482"In between, there was a variety of rigorously controlled experiments.",0,9111.83,10085.83"Therefore, in light of this trend, it seems an appropriate time to reopen the question about what the ultimate purpose of visualization is and how it should be evaluated.",4,3307.545,4281.545p,,,One potential claim is: The purpose of visualization is insight.,4,10127.202,11101.202The purpose of visualization evaluation is to determine to what degree visualizations achieve this purpose.,3,7172.525,8146.525p,,,"If this claim is true, then evaluating visualizations should seek to determine how well visualizations generate insight.",2,7835.104,8809.104"Measuring insight would enable the direct comparison of visualization design alternatives, or the comparison against an insight goal.",2,6774.762,7748.762"But what, exactly, is insight?",1,13953.55,14927.55How can it be measured and evaluated?,0,4645.704,5619.704Do current approaches for evaluating visualizations provide measures of insight?,0,6243.905,7217.905p,,,Defining insight,0,14768.702,15742.702p,,,Insight has been commonly stated as the broader purpose of information visualization by many authors.,1,9440.06,10414.06The recent emphasis on visual analytics has stimulated an interest in better understanding the purpose of visualization and rigorously evaluating progress toward that purpose.,0,3348.608,4322.608Insight seems to capture the intuitive notion of visualization's purpose.,0,8091.918,9065.918"However, for the most part, the definition of insight remains fairly informal, making success difficult to evaluate.",3,5418.948,6392.948p,,,"A default and implicit definition is to equate insight with user tasks, such as finding extreme values.",0,7415.188,8389.188"That is, the answers to questions about the data constitute insight.",0,10218.988,11192.988"Yet, truly measuring the insight-generating capability of visualizations will require a more in-depth examination of insight itself.",1,11385.192,12359.192"Perhaps researchers have resisted the temptation to formally define insight, believing that any formal definition would either be too restrictive to capture its essence or too general and vague to be useful.",1,6330.718,7304.718"Hence, instead, it might be helpful to identify essential characteristics of insight, and then consider whether measurement methods capture those characteristics.",2,5740.287,6714.287p,,,"In the spirit of gaining understanding of insight, here I list some of its important characteristics.",0,4172.294,5146.294p,,,Complex.,1,8711.07,9685.07"Insight is complex, involving all or large amounts of the given data in a synergistic way, not simply individual data values.",2,5530.462,6504.462p,,,Deep.,1,2295.806,3269.806"Insight builds up over time, accumulating and building on itself to create depth.",2,4246.37,5220.37"Insight often generates further questions and, hence, further insight.",2,7945.256,8919.256p,,,Qualitative.,2,4056.548,5030.548"Insight is not exact, can be uncertain and subjective, and can have multiple levels of resolution.",3,5985.27,6959.27p,,,Unexpected.,2,1046.031,2020.031"Insight is often unpredictable, serendipitous, and creative.",3,5624.381,6598.381p,,,Relevant.,1,4308.581,5282.581"Insight is deeply embedded in the data domain, connecting the data to existing domain knowledge and giving it relevant meaning.",2,6299.874,7273.874"It goes beyond dry data analysis, to relevant domain impact.",0,5470.503,6444.503p,,,"Typically, the most interesting or important insights are those that rank highly in each of the previous characteristics.",2,3075.608,4049.608"For example, complexity is determined by how much data is involved in the insight.",1,9369.506,10343.506"Simple insights, such as finding minimum or maximum values, involve simple answers that require only one data value.",0,5807.191,6781.191"Recognizing a normal distribution of values is more complex, and might involve approximate parameters of distribution shape.",0,8341.046,9315.046Knowing that the distribution of values looks like the histogram in Figure 1 is even more complex because this understanding involves more data and thus reveals the peculiarities in its shape.,0,2674.481,3648.481p,,,Evaluating visualizations,0,11012.656,11986.656p,,,"A variety of visualization evaluation methods exist, including empirical methods such as controlled experiments, usability testing, and longitudinal studies, and analytical methods such as heuristic evaluation and cognitive walk-throughs.",1,6684.54,7658.54Controlled experiments are increasingly common in the literature because the controlled nature of that method best enables researchers to rigorously measure and conclusively compare visualizations.,1,4218.25,5192.25"Hence, this article examines the capability of the controlled experiment method to measure insight.",3,5643.888,6617.888p,,,Controlled experiments on benchmark tasks,1,12757.904,13731.904p,,,The use of controlled experiments on benchmark tasks is the primary method for rigorously evaluating visualizations.,1,10832.55,11806.55"In this procedure, objective metrics are captured while a sample set of human participants uses the targeted visualizations to perform a series of benchmark tasks.",0,5331.593,6305.593An example benchmark task might ask users to find the maximum value in the data set.,0,7097.061,8071.061p,,,The two primary independent variables are the visualization design alternatives and benchmark tasks.,0,8296.843,9270.843"Others common independent variables include the data set size or type, or the user class.",0,6747.641,7721.641"Primary dependent measures are the user performance time to complete the task, and the accuracy of their task response (for example, the correctness of their answer).",1,3744.745,4718.745Other dependent measures include behavioral metrics such as the number of mouse click actions.,1,7175.045,8149.045"Researchers can then relatively compare the targeted visualizations based on the captured metrics to determine, for example, which visualization design resulted in the fastest user performance for a given benchmark task.",0,7482.57,8456.57p,,,The usefulness of this method as a way to measure insight hinges entirely on whether the benchmark tasks and metrics adequately represent insight.,1,10819.07,11793.07"However, benchmark tasks have four fundamental problems that are in direct opposition to the desired characteristics of insight as listed previously.",3,6628.249,7602.249p,,,They must be predefined by test administrators.,0,2631.381,3605.381"Users must precisely follow specific instructions during the experiment, leaving little room for unexpected insight.",0,4723.08,5697.08p,,,They need definitive completion times.,0,3114.854,4088.854"Task times must be short, typically under one minute, to examine a large number of tasks or repetitions.",0,5412.577,6386.577Stopping the timer clock at exactly the moment the user finds the answer leaves little room for deep insight.,0,3541.1,4515.1p,,,They must have definitive answers that measure accuracy.,0,5569.998,6543.998"Multiple choice task questions enable objective mechanical (and even automated) scoring and treat answer correctness as Boolean, leaving little room for qualitative insight.",0,3497.294,4471.294p,,,They require simple answers that users can easily specify.,0,7441.169,8415.169"Users click on the desired data object or state its unique identifier value, leaving little room for complex and relevant insight.",0,6771.906,7745.906p,,,"Because of these problems, experimenters are forced to use overly search-like tasks that don't represent insight well.",2,11625.755,12599.755Two examples of common benchmark tasks in evaluations in the information visualization literature are: find the data record that meets the following attribute criteria and find which record has the maximum value of attribute X.,1,10893.718,11867.718"While such tasks might indicate that users can quickly locate or discriminate individual values, it seems far too simplistic and constrained to provide a useful indication of the kind of insight that visualization is trying to achieve.",0,7406.428,8380.428"In many cases, a simple query, sorting function, or summary statistics might solve the benchmark task faster.",0,3296.337,4270.337p,,,"Furthermore, predefined benchmark tasks are troublesome for several reasons.",1,6490.948,7464.948They force users into a line of thought that they might not otherwise take.,0,5035.593,6009.593"They place an undo burden on evaluation designers, who are susceptible to bias and have little structured guidance to overcome it.",0,2420.436,3394.436The choice of tasks and the phrasing of task questions can introduce bias toward one of the visualization designs.,1,6021.233,6995.233Benchmark tasks lack completeness.,0,9785.844,10759.844"A visualization might perform well at certain tasks, but at what cost?",0,6689.231,7663.231What other tasks will suffer?,0,8754.82,9728.82"Finally, because the tasks must be predefined, the experiment's results are limited to only the tasks that evaluators chose.",0,8649.543,9623.543"To generalize the results beyond simple benchmark tasks, researchers make the implicit claim that complex tasks will be built upon simple tasks, like a hierarchical task decomposition.",1,8752.533,9726.533"Hence, if a visualization can efficiently support a variety of such simple tasks, then complex tasks will also be efficient.",1,7919,8893p,,,The counterargument to this claim is twofold.,0,2848.815,3822.815"First, the efficiency of the simple benchmark tasks is often due to specific visualization interface features that don't generalize to more complex tasks.",0,11018.124,11992.124"For example, one visualization might use larger text labels for reading detailed data values, or perhaps includes a dynamic query slider that is a particularly good match for the criteria-finding tasks.",0,6902.622,7876.622"Second, such a clear task decomposition does not exist as yet, and so it's unclear which simple tasks should be tested to support more complex tasks.",0,5809.474,6783.474The chasm between simple single-data-value tasks and complex synergistic tasks seems large.,0,8301.134,9275.134"For example, treemaps make it easy to find the largest rectangle, but probably distorts the recognition of rectangle size distribution.",0,1657.204,2631.204Is a data distribution recognized by performing a large number of value-reading tasks?,0,6409.925,7383.925p,,,A further confusion in the interpretation of benchmark experiment results is the tradeoff between performance and accuracy.,0,3847.136,4821.136What does it mean if a visualization has better performance time but lesser accuracy than another visualization?,0,4616.75,5590.75"Was the visualization really faster, or were participants giving up and guessing?",0,4197.421,5171.421"Attempting to equalize accuracy by forcing users to continue until correctly completing the task, leads to a trial-and-error approach by users.",1,3404.719,4378.719"Alternatively, filtering incorrect responses in the analysis ignores important information.",0,866.62,1840.62None of these options corresponds well to insight.,0,7424.323,8398.323p,,,Controlled experiments on benchmark tasks can provide a rigorous method for examining specific perceptual effects and specific tasks from a usability specification.,1,7363.254,8337.254"However, it does not provide a satisfying representation of insight capability.",1,5579.758,6553.758p,,,We need new evaluation methods that attempt to measure insight more directly.,3,6717.19,7691.19We also need to preserve the positive aspects of the controlled experiment methodology to enable rigorous comparison of visualizations.,0,3107.205,4081.205How can we modify or tweak the controlled methodology to better capture the desired characteristics of insight?,0,5733.318,6707.318p,,,Toward insight: more complex benchmark tasks,1,13879.869,14853.869p,,,An initial step is to include benchmark tasks of greater complexity in the experimental protocol.,1,8529.986,9503.986"For example, we can ask users to characterize the distribution of data values, and include a multiple choice set of answers such as _normal,_ _uniform,_ _linearly increasing,_ and so on.",0,4507.554,5481.554We can carefully craft similar questions for correlations or other types of patterns.,0,3701.837,4675.837"Another possibility is estimation tasks, in which users estimate various metrics, such as coverage or cluster density, without actually counting.",0,3934.965,4908.965"In this case, the correctness measure can be a real value instead of Boolean.",0,5125.321,6099.321p,,,"While these benchmark tasks still suffer from some of the problems identified previously, they at least begin to test more synergistic, complex tasks that involve some uncertainty.",0,5816.189,6790.189These types of tasks generally support visualization overviews rather than detail views.,0,5754.51,6728.51p,,,Forcing users to interpret the visualization into a textual answer ensures that they have developed their mental model of the data.,0,5424.994,6398.994"However, forcing users to articulate their answers, by not providing them with multiple-choice answers, might be difficult to score.",0,5423.834,6397.834"Alternatively, providing multiple-choice answers can lead users into a process of elimination, creating a challenge for the evaluation designer to provide careful wording of the possible answers.",0,3848.657,4822.657"Phrases with visual metaphors, such as _densely clustered_ or _higher,_ might bias toward a particular visualization and should be avoided.",0,5658.692,6632.692p,,,"The difficulties with this method are longer task times, greater variability in task times and correctness, and greater difficulty in designing isomorphic tasks (different instances of the same task type for use in repeated measures).",0,4062.081,5036.081"Together, these problems generally mean that researchers must test more participants to get statistically significant results.",0,2236.435,3210.435p,,,Toward insight: eliminating benchmark tasks,0,13842.391,14816.391p,,,A more radical step is to eliminate the pesky benchmark tasks from the protocol entirely.,1,6166.709,7140.709This method's fundamental concept is to change the benchmark tasks from an independent to a dependent variable.,2,5852.588,6826.588"Hence, instead of instructing users in exactly what insights to gain, researchers observe what insights users gain on their own.",2,5420.952,6394.952p,,,"This method involves the following key innovations: an open-ended protocol, a qualitative insight analysis, and I an emphasis on domain relevance.",2,4665.932,5639.932p,,,"With an open-ended protocol, users explore the data in a way that they choose.",0,4923.44,5897.44Giving the users a chance to think of initial questions helps them get started.,0,2485.522,3459.522But soon they go beyond those initial questions in depth and unexpectedness.,0,2299.365,3273.365Users are instructed to explore the data and report their insights until they feel that they have learned all that they can from the data.,1,6400.868,7374.868p,,,"Using qualitative insight analysis, users verbalize their findings in a think-aloud protocol so that evaluators can capture the users' insights.",1,5672.216,6646.216Each finding that the user reports is marked as an insight occurrence.,0,6326.235,7300.235"Then, for each insight, a coding method quantifies various metrics such as insight category, complexity, time to generate, errors, and so on.",0,8217.566,9191.566"For example, depth could be coded on a scale of 1 to 5, where 1 represents a simple obvious fact in the data and 5 represents a deep inference that integrates multiple data types.",0,2920.204,3894.204Insight categories can be developed by examining the entire collection of insights for common clusters.,0,5311.523,6285.523Usability and human factors experiments commonly use such coding methods.,0,2554.157,3528.157"Coding converts qualitative data to quantitative and is inherently more subjective, but supports the qualitativeness of insight.",1,5153.09,6127.09Significant objectivity can be maintained through rigorous coding practices.,0,2334.324,3308.324p,,,"To emphasize domain relevance, experiment participants should be users from the target domain.",0,4428.982,5402.982Independent domain experts acting as coders provide critical metrics for the value or importance of the reported insights in the domain.,1,3220.968,4194.968"Experimenters should pay special attention to cases where the user goes beyond dry data analysis, and makes domain-specific inferences and hypotheses.",1,2351.221,3325.221p,,,The key advantage of eliminating benchmark tasks is that it reveals what insight visualization users gained.,4,11662.955,12636.955"Researchers can then compare visualizations on insight-related measures such as the number of insights, the categories of insights generated, the speed at which insights were generated, and the domain value of the insights.",1,4484.291,5458.291These measures are closely related to the fundamental characteristics of insight as identified previously.,0,5010.136,5984.136Researchers can also compare the insights that users gained with the insights that they expected users to gain.,0,6012.262,6986.262"For example, which insights did users fail to discover with the visualization?",0,6947.454,7921.454p,,,"Furthermore, since the protocol shares some similarities to that of formative usability studies, researchers can simultaneously collect a wealth of usability data and correlate it to the insight results.",0,4764.051,5738.051"For example, which features of the visualization helped achieve insight, and which caused problems for the users?",0,7516.028,8490.028This directly leads to visualization refinement and improvement.,0,5382.682,6356.682p,,,"The difficulties with this method include the need for potentially long training and trial times depending on data and domain complexity; more effort by the experimenters to capture and code results; motivated, domain knowledgeable users who will not merely follow instructions but generate insight in a self-directed manner; and domain experts to assist in coding results along with visualization experts.",0,4878.366,5852.366p,,,"In general, these problems are not fundamental, in that experimenters can overcome them given sufficient resources.",0,2844.292,3818.292"At the same time, however, this method provides the resource advantage of relieving the experimenter from having to design benchmark tasks, a surprisingly difficult job.",0,3758.709,4732.709"As with the previous method, greater variance in the results is also a problem.",0,2187.642,3161.642"Philosophically, with this method we must be willing to live with somewhat more subjective results.",1,2428.445,3402.445p,,,Onward,0,0.677,974.677p,,,"In practice, both types of controlled experiments are needed.",5,1853.237,2827.237Benchmark task experiments help identify specific low-level effects.,2,3603.951,4577.951Eliminating benchmark tasks provides a much richer (more insightful) view of the broader insight capability of visualizations.,2,6007.61,6981.61"If combining both approaches into a single experiment, the benchmark tasks should not precede the open-ended portion.",2,4443.312,5417.312"Otherwise, user thinking will become constrained by the benchmark tasks when moving on to the open-ended portion.",0,4493.844,5467.844p,,,"Future steps should take a more rigorous and comprehensive approach to comparing these methods, perhaps running the same experiment with each method to see how they differ.",1,1964.433,2938.433Experimenters can also examine and adapt other uncontrolled evaluation methods to better gauge insight.,0,4949.467,5923.467"These new methods can provide better measures of visualization insight, and ultimately determine whether visualizations are achieving their grand purpose.",1,5894.206,6868.206